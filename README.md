# llm-inference-lab
Handy and ready to run inference code for LLM extracting that last bit of performance. Ultimately, it's all about those tokens/second!!!
